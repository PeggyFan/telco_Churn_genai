Notes

1. Generating synthetic data using LLM
- Customer feedback. Passing several customer attributes to the OpenAI API
- Tokens used were ~ 1.5-1.8M
- Cost ~$0.97
- Time taken: ~2 hours

2. Evaluating LLM generated feedback
Option 1 â€” 
Use Embedding Similarity: Provide examples of a typical feedback for churned customer and customer who stayed.

Use transformer model to calculate the similarity between all the feedback and those two canonical archetypes. Then calculate "churn alignment" with 

churn alignment = similarity score to churn - similarity score to stayed 
Then if the churn alignment is 
negative, the feedback is leaning towards stayed
positive, the feedback is leaning towards churned
zero, the feedback is neutral

Then we calculate the correlation between the churn alignment score and the churn outcome (1 for churned, 0 for stayed).
The correlation is 0.632

Option 2 - 
Use the pre-trained Bert model to analyze the sentiment of the feedback, giving a label (and a score). Given that negative sentiment = 1 and positive = 1, calculate the correlation between those sentiment score and churn outcome. 
This correlation is 0.877 

Compare the two approaches!




Feature engineering option based on the LLM generated feedback:

In the case of customer churn, there are two ways I can leverage the feedback data.

1. Sentiment based on feedback
I used the transformer model (Bert) trained on sentiment analysis to generate sentiment label (as a new feature) based on the `feedback` column.

I applied a batch processing method to avoid memory/speed issue. Because the telco dataset is only ~7500 rows, this did not take long. This I called dataset V1.

But imagine if you have 7.5M rows. What if you cannot afford using LLM to make zero-shot or few-shot predictions for a very large dataset? If your dataset is 1M plus that uses a large number of tokens, they can cost hundreds if not thousands of dollars each run.

So I tried the option of only processing a subset of your data using the models to generate the labels, which produces a pseudo gold dataset. Then train my own student (smaller) model using that pseudo dataset, and make predictions for the rest of the original data. 
I used again the Bert model as the student model, which trained on half of the data. It then generated sentiment labels for the rest of the dataset. But one can easily choose a simpler classifier for the student model as well.
This I called dataset V2.

2. Defined categories based on feedback 
I came up with five general themes that a feedback can touch upon: "billing", "customer service", "product quality", "technical issues", "other."

I tried to replicate the same approach to generate defined categories as a new feature, but it was taking much longer. I decided to stop.

3. Unsupervised clustering/topics based on feedback
Using embeddings created from the `feedback` column, I used `umap` to create 5 clusters. I chose five to align with the defined categories I tried above. Those 5 clusters then are the features I added to the data. This is dataset V3.

So this project does not focus on model selection; I ran only two models- Logistic and Random Forest. I was more interested in the data augmentation aspect of the problem given that we can add synthetic data such as feedback to the original dataset. 

The best performing models were Random Forest models (no surprise) on dataset V1 and V2.
This is very interesting, because V1 is the dataset where all the sentiment labels are generated by the sentiment model, and V2 is the student model approach. While I don't have a clear answer on what percentage of the data needed to be alloted for a student model to train and be affect, or what types of model make a good student model, it is promising to see its performance can be on par with the feature generated directly by the LLM.

What is key is how to translate the model predictions into business context that stakeholders can understand. I created a business ROI calculator 